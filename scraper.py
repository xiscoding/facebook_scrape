import os
import re
import pandas as pd
from bs4 import BeautifulSoup
import pandas as pd
import re

# Global path for the HTML file
HTML_FILE_PATH = r"/home/xdoestech/facebook_scrape/htmls/facebook-xhesnschel-2025-01-03-PoLeo7pj/connections/friends/people_you_may_know.html"

def looks_like_a_name(text):
    """
    Returns True if the text looks like a person's name, subject to these conditions:
      1) The text is a string (not None or another type).
      2) Contains only letters (including accented), apostrophes, hyphens,
         periods, or spaces (no digits or disallowed punctuation).
      3) Has at most 4 words.
      4) The first and last word are capitalized (title case).
    """
    if not isinstance(text, str):
        return False
    
    # Strip leading/trailing spaces and split into words
    words = text.strip().split()
    
    # 1) Reject if more than 4 words
    if len(words) > 4:
        return False

    # 2) Reject if the first or last word are not capitalized (title case)
    #    .istitle() checks that the first character is uppercase and the rest are lowercase
    if not (words[0].istitle() and words[-1].istitle()):
        return False
    
    # 3) Check only allowed characters with a regex
    #    ^ start of string
    #    [A-Za-zÀ-ÖØ-öø-ÿ'.\- ]+ => letters (A–Z or a–z, including accented), apostrophes, hyphens, periods, or spaces
    #    $ end of string
    pattern = r"^[A-Za-zÀ-ÖØ-öø-ÿ'.\- ]+$"
    if not re.match(pattern, text.strip()):
        return False
    return True

def clean_dataframe_by_regex(df):
    """
    Removes rows from 'df' where the 'Name' column does not match a regex
    that indicates 'Name-like' strings.
    """
    # Apply our helper function to each entry in the 'Name' column
    mask = df['Name'].apply(looks_like_a_name)
    return df[mask]

def extract_data_from_html(HTML_FILE_PATH:str):
    """
    Reads HTML, parses it with BeautifulSoup, and returns:
      - the name in the 'Generated by ...' text,
      - the date range found after 'Contains data you requested from ... to ...',
      - and a list of text from all leaf <div> elements.
    """
    with open(HTML_FILE_PATH, 'r', encoding='utf-8') as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, 'html.parser')

    generated_by_name = None
    date_range = None
    leaf_texts = []

    # 1) Look for "Generated by ..." and "Contains data you requested from ... to ..."
    for div in soup.find_all('div'):
        full_text = div.get_text(strip=True)
        
        # Attempt to capture the name in "Generated by Xatoshi Marabunta on ..."
        if "Generated by" in full_text:
            match_name = re.search(r"Generated by\s+(.+?)\s+on", full_text)
            if match_name:
                generated_by_name = match_name.group(1)
        
        # Attempt to capture date range in "Contains data you requested from ... to ..."
        if "Contains data you requeste" in full_text:
            match_dates = re.search(r"from\s+(.+?)\s+to\s+(.+)", full_text)
            if match_dates:
                start = match_dates.group(1).strip()
                end = match_dates.group(2).strip()
                date_range = f"{start} to {end}"

    # 2) Gather text from all leaf <div> elements (no nested <div>)
    for div in soup.find_all('div'):
        if not div.find('div'):
            text = div.get_text(strip=True)
            if text:
                leaf_texts.append(text)

    return generated_by_name, date_range, leaf_texts

def build_dataframe(HTML_FILE_PATH:str):
    """
    Builds a pandas DataFrame with 3 columns:
      'Generated By', 'Data Range', 'Name'
    Each 'Name' is one of the leaf <div> texts extracted.
    """
    gen_name, drange, names_list = extract_data_from_html(HTML_FILE_PATH)

    # Create one row per name
    rows = []
    for person_name in names_list:
        rows.append({
            'Generated By': gen_name,
            'Data Range': drange,
            'Name': person_name
        })

    df = pd.DataFrame(rows)
    return df

def scraper(HTML_FILE_PATH:str) -> pd.DataFrame:
    """
    Creates a cleaned DataFrame from the HTML file
    by building the base DataFrame and removing entries
    that don't look like names.
    """
    df = build_dataframe(HTML_FILE_PATH)
    df_cleaned = clean_dataframe_by_regex(df)
    return df_cleaned

def scrape_and_append_to_csv(html_path: str, csv_path: str = "friends_list.csv") -> None:
    """
    Uses the scraper to parse an HTML file and appends the resulting
    DataFrame to the specified CSV file. If the CSV file does not exist,
    it is created with headers. If it does exist, new rows are appended
    without headers.

    :param html_path: Path to the HTML file to parse.
    :param csv_path: Path to the CSV file where we want to append. Default "friends_list.csv".
    """
    # Scrape the data
    df_cleaned = scraper(html_path)

    # Check if the CSV already exists
    file_exists = os.path.isfile(csv_path)

    # Append to CSV, adding header only if the file doesn't exist
    df_cleaned.to_csv(
        csv_path,
        mode='a',            # append mode
        index=False,
        header=not file_exists
    )

if __name__ == "__main__":
    df = build_dataframe(HTML_FILE_PATH)
    df_cleaned = clean_dataframe_by_regex(df)
    print(df_cleaned)

    # If you want, you can save to CSV:
    # df.to_csv("output.csv", index=False)
